#在聚类分析中，类别的个数及个体标签本身并不存在，只是根据个体特征的相似性形成“合理”的聚集，并无“正确答案”参考，故其属于无监督学习(unsupervised learning)。
#聚类分析的应用场景:精准营销
#合理的聚类方式应使得同一族群内的观测尽可能地“相似”，但不同族群之间有明显区分
'''
对于两个p维矩阵,它们之间的距离可以用以下几种方式来衡量:
欧氏距离
马氏距离是“消除了量纲和变量相关性” 的欧式距离
马氏距离公式是:d(x,y)=√( (x-y)T S^(-1) (x-y) )
其中S为样本协方差矩阵
Minkowski距离:Minkowski距离是欧氏距离的推广形式
d(x,y)=( ∑|xi-yi|^p )^(1/p)
当p=1时，Minkowski距离即为曼哈顿距离
当p=2时，Minkowski距离即为欧氏距离
当p趋近于无穷大时，Minkowski距离即为切比雪夫距离
(需要根据数据特点选择 “差异计算方式” 的场景（比如文本词频差异用p=1，空间距离用p=2）。)
Canberra距离:Canberra距离是Minkowski距离的一种变形，适用于数据值范围差异较大的情况
d(x,y)= ∑ |xi-yi| / (|xi| + |yi|)
是基于 “相对差异” 的测度,对非负数据（比如计数、频率、浓度）更友好,对 “0 附近的小数值差异” 更敏感

'''

'''
聚类方法分类:
层次聚类/系统聚类(hierarchical clustering):

通过构建层次树状结构来表示数据的聚类关系,包括凝聚型(agglomerative)和分裂型(divisive)两种方法

凝聚法:从每个观测值开始,逐步将距离最近的簇合并,直到达到预定的簇数或距离阈值
在一开始每个数据点都是一个聚类,所以有N个据类,其中N是数据点的数量
迭代:每一步,找到最近两个聚类并合并,因此聚类的数量减少一个
注意:距离可以用不同的方法来计算,如单链接(single linkage)、全链接(complete linkage)或平均链接(average linkage
结束:最后只剩下一个包含所有数据点的聚类
层次聚类过程的结果可以利用图表展示为系统树图（Dendrogram），用来展示层次聚类的每一个步骤及其结果，包括合并族群带来的距离的变化

分离法:从所有观测值开始,逐步将簇分裂成更小的簇,直到达到预定的簇数或距离阈值
开始:所有数据点都属于一个大的聚类。
迭代：在每一步，选择一个聚类并将其分割为两个子聚类。
结束：最后每个数据点都成为自己的聚类。
凝聚法每一步需要合并“距离最小的两个族群”，不同族群间距离的定义方法决定了不同的聚类结果。

层次聚类的类型
连接法和Ward法
连接法:基于簇间距离的最小值、最大值或平均值来决定簇的合并
连接法中又分为简单链接,完全链接,平均连接,质心连接
简单连接（Single linkage）/最近邻方法（Nearest neighbor method）定义族群间的距离为两族群中相隔最近的两个体间的距离。
简单链接法中怎么样计算迭代后的族群间距离?
比如说,A和B合并后AB和C的距离怎么计算?一般是取AB中距离C最近的点与C的距离作为AB和C的距离
完全链接（Complete linkage）/最远邻方法（Farthest neighbor method）定义族群间的距离为两族群中相隔最远的两个体间的距离。
第一步是根据初始距离矩阵划分，无论用哪一种层次聚类结果是一样的
完全连接法和简单链接法的区别是计算迭代后的族群间距离的方法有区别
完全链接法下A和B合并后AB和C的距离怎么计算?一般是取AB中距离C最远的点与C的距离作为AB和C的距离
平均连接法（Average linkage）中，两族群之间的距离定义为nA个A集合点和nB个B集合点产生的所有nA*nB个距离数值的平均：
质心法（Centroid method）中，两族群的距离定义为两族群各自的质心（Centroid），即样本均值向量，之间的欧式距离：
在族群合并后，新族群的心由合并后族群内所有样本点求平均得到.
在质心法中,族群A包含更多样本,而族群B包含不那么多样本,则新的质心可能由原来的A质心主导,B的贡献被忽略
为了避免这种情况，我们可以用两族群质心连线的中点作为合并后新组别的质心

倒置现象:
如果两个族群合并之后，下一步合并时的最小距离反而减小，这种情况称为倒置（Reversal/Inversion），在系统树图中表现为交叉（Crossover）现象。
在一些层次聚类方法中，如简单连接、完全连接和平均连接，倒置不可能发生，这些距离的度量是单调的（monotonic）。

Ward法（Ward’s method）/方差平方和增量法（Incremental sum of squares） 由合并前后的族群内方差平方和的差异定义距离：
IAB=SSEAB-(SSEA+SSEB) 

层次聚类族群个数的选择原则:
可根据经验或业务解释预先设定
也可由数据驱动：从系统树图中于给定距离水平下“切分”树图得到对应族群
我们通常寻找合并组别时较大的距离变化的节点

层次聚类的缺点:一旦个体被分入一个族群，它将不可再被归入另一个族群

总结层次聚类的步骤:
建立n个初始族群，每个族群中只有一个个体
计算n个族群间的距离矩阵
合并距离最小的两个族群，计算新族群间的距离矩阵
如果组别数为1，转步骤5；否则转步骤3
绘制系统树图
选择族群个数

非层次聚类(flat clustering):
分割法:分割法中最常用的方法为k-均值（k-Means）法
k-均值法试图寻找k个族群(G1,G2,Gk)的划分方式，使得划分后的族群内方差（within-group sum of squares，WGSS）最小。
WGSS公式:先计算每一组的SSE(组内均值平方差), 然后把所有组的SSE加起来就是WGSS
但是,K-均值法有几个问题:
第一步中的初始中心点怎么确定？随便选吗？不同的初始点得到的最终聚类结果也不同吗？
第二步中点之间的距离用什么来定义？
第三步中的所有点的均值（新的中心点）怎么算？

k-均值法的步骤:
选定k个“种子”（Cluster seeds）作为初始族群代表
每个个体归入距离其最近的种子所在的族群
归类完成后，将新产生的族群的质心定为新的种子
重复步骤2和3，直到不再需要移动

K怎么选择？ 需要选择一个使得WGSS足够小（但不是最小）的k值
画图:横坐标是聚类的族群数量,纵坐标就是WGSS
用WGSS的碎石图来寻找最优的k，通常选取拐点(knee point)为最优的k。

选取k个初始种子的方法:
在相互间隔超过某指定最小距离的前提下，随机选择k个个体
选择数据集前k个相互间隔超过某指定最小距离的个体
选择k个相互距离最远的个体
选择k个等距网格点，可能不是数据集的点


注意:
k-均值法对初始种子的选择比较敏感。可尝试再不同的种子选择下多次进行聚类。
如果不同的初始种子造成了聚类结果很大的不同，或是收敛的速度极其缓慢，也许说明数据可能原本并不存在天然的族群。

k-均值法也可以作为层次聚类的一种改进:
我们首先用层次聚类法聚类，以各族群的质心作为k-均值法的初始种子。
这样就在层次聚类的基础上，使得个体有被重新分配到其他族群的机会。
'''
