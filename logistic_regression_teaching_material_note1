'''
逻辑回归的数学原理
建立决策边界,对样本进行分类,
使用Sigmoid函数：该函数可以将取值为(-∞, +∞)的数转换到(0,1)之间,例如倘若y=3,那个通过Sigmoid函数转换后,f(y)就变成了1/(1+e^-3)=0.95了,这就可以作为一个概率值使用了。
输入数据(样本的特征向量x)通过假设函数h(x)得到输出结果(模型的预测结果)
逻辑回归的预测结果是概率h(x),取值属于零到一
当h(x)趋近于1,样本更可能是正例,h(x)趋近于0,样本更可能是负例
在使用逻辑回归领测样本的类别时会提前设置一个國值P,用来控制分类分类的界限,也就是说,h(x)大于等于P,就是正例,h(x)小于P就是负例
Sigmoid函数的公式：
f(y)=1/(1+e^-y)
其中y=w0x0+w1x1+w2x2+...+wnxn(实际上就是Y=w.T·X的形式)
其中x0=1,w0为偏置项
逻辑回归的损失函数
J(w)=-(1/m)∑[y(i)log(h(x(i)))+(1-y(i))log(1-h(x(i)))]
(也就是交叉熵损失函数)
简化:L(y,h(x))=-[ylog(h(x))+(1-y)log(1-h(x))]
其中m为样本数量,y(i)为样本i的真实标签,h(x(i))为样本i的预测概率
逻辑回归的参数优化
通过梯度下降法来优化损失函数,更新参数w的公式为：
wj:=wj-α∂J(w)/∂wj
其中α为学习率,∂J(w)/∂wj为损失函数对参数wj的偏导数

总结:线性回归:z=wx+b
逻辑回归=线性回归+Sigmoid函数
sigmoid函数:y=1/(1+e^-z)
逻辑回归:y=1/(1+e^-(wx+b))

逻辑回归的损失函数公式是怎么算出来的:
逻辑回归的损失函数是基于最大似然估计(Maximum Likelihood Estimation, MLE)推导出来的。
假设样本输出为0或1两类,那么,有,
P(y=1|x;w)=h(x),P(y=0|x;w)=1-h(x)(为什么呢?因为一般设定正例的概率为p也就是h(x),负例的概率为1-p也就是1-h(x))
因此,可以将上述两种情况合并为一个公式:
P(y|x;w) = h(x)^y *(1 - h(x))^(1 - y)
得到了y的概率分布函数表达式,可以用似然函数L(w)表示所有样本的联合概率:
L(w) = ∏ P(y(i)|x(i);w) = ∏ [h(x(i))^y(i) * (1 - h(x(i)))^(1 - y(i))]
但是呢,上面的似然函数得到的结果是某一个样本的代价,m个样本的平均代价是:
j(w) = (1/m) * ∑ -log(P(y(i)|x(i);w))
将P(y(i)|x(i);w)代入上式,得到:
j(w) = (1/m) * ∑ -[y(i)log(h(x(i))) + (1 - y(i))log(1 - h(x(i)))]
这就是逻辑回归的损失函数公式。
实际上,从推导公式可以看到,
样本为正例时,代价cost随预测值h(x)的增大而减小;
样本为负例时,代价cost随预测值h(x)的减小而减小。
两部分的代价加起来就是逻辑回归的总损失函数。
总损失函数先变小后变大,
所以逻辑回归的损失函数是一个凸函数,
可以通过梯度下降法找到全局最优解。

逻辑回归也有正则化项,用来防止过拟合:
J(w)=-(1/m)∑[y(i)log(h(x(i)))+(1-y(i))log(1-h(x(i)))] + (λ/2m)∑wj^2
其中λ为正则化参数,控制正则化项的强度。
当λ较大时,模型会更倾向于选择较小的权重,从而降低模型复杂度,防止过拟合。

'''
#Sigmoid函数的实现
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
#逻辑回归的预测函数
def predict(X, w):
    z = np.dot(X, w)#注意是点乘!!!
    return sigmoid(z)
#逻辑回归的损失函数
def compute_loss(y, h):
    m = y.shape[0]#样本数量
    loss = - (1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
    return loss
#逻辑回归的梯度下降优化
def gradient_descent(X, y, w, alpha, num_iterations):
    m = y.shape[0]
    for i in range(num_iterations):
        h = predict(X, w)
        gradient = (1/m) * np.dot(X.T, (h - y))
        w -= alpha * gradient
        if i % 100 == 0:
            loss = compute_loss(y, h)
            print(f'Iteration {i}, Loss: {loss}')
    return w
#解释一下梯度下降优化算法的原理
'''
梯度下降法是一种用于优化函数的迭代算法,旨在找到函数的局部最小值。在机器学习中,梯度下降法常用于最小化损失函数,从而优化模型参数。
基本原理：
1. 初始化参数：选择一个初始点作为参数的起始值。
2. 计算梯度：在当前参数位置计算损失函数的梯度(偏导数)。梯度指向函数值增加最快的方向,因此我们需要沿着梯度的反方向更新参数。
3. 更新参数：根据学习率(α)调整参数,公式为：w := w - α * ∇J(w),其中∇J(w)是损失函数J对参数w的梯度。
4. 重复迭代：重复步骤2和3,直到满足停止条件(如达到最大迭代次数或梯度足够小)。
梯度下降法的关键在于选择合适的学习率。如果学习率过大,可能会导致参数更新过度,错过最优解；如果学习率过小,收敛速度会很慢。
'''

#Sigmoid函数的绘制
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(-10, 10, 100)#生成-10到10之间的100个点
y=1.0/(1+np.exp(-x))#计算Sigmoid函数值
plt.plot(x,y)
plt.title('Sigmoid Function')
#plt.show()

#逻辑回归的完整实现示例
#生成一些模拟数据
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=42)
#添加偏置项
X = np.hstack((np.ones((X.shape[0], 1)), X))
#初始化参数
w = np.zeros(X.shape[1])
#设置超参数
alpha = 0.01    #学习率
num_iterations = 1000  #迭代次数        
#训练逻辑回归模型
w = gradient_descent(X, y, w, alpha, num_iterations)
#预测
h = predict(X, w)
#将概率转换为类别标签           
predictions = (h >= 0.5).astype(int)
#计算准确率
accuracy = np.mean(predictions == y)
print(f'Accuracy: {accuracy}')
#绘制决策边界
plt.scatter(X[:, 1], X[:, 2], c=y, cmap='viridis')
x_values = np.array([np.min(X[:, 1]), np.max(X[:, 1])])
y_values = -(w[0] + w[1] * x_values) / w[2]
plt.plot(x_values, y_values, color='red')   
plt.title('Logistic Regression Decision Boundary')
plt.show()
#以上为vscode自动补全内容